{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkf-c2qo6I5h"
      },
      "source": [
        "# Music classification\n",
        "* 출처 : https://github.com/ZainNasrullah/music-artist-classification-crnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZRhEvqshjE4"
      },
      "source": [
        "# 라이브러리 설치 및 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJj90LwllcAg"
      },
      "source": [
        "!pip install numpy\n",
        "!pip install librosa\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IKElYb4hmR7"
      },
      "source": [
        "import os\n",
        "from os.path import isfile\n",
        "import gc\n",
        "import dill\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import RandomState\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.utils import shuffle\n",
        "from scipy import stats\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Reshape, Permute\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.recurrent import GRU, LSTM\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCzYCw4Nem2X"
      },
      "source": [
        "# 데이터셋 관련 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anZneghu7q3-"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.get_cmap('Blues')):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def plot_history(history, title=\"model accuracy\"):\n",
        "    \"\"\"\n",
        "    This function plots the training and validation accuracy\n",
        "     per epoch of training\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title(title)\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOLcAqMT6n4F"
      },
      "source": [
        "def create_dataset(artist_folder='artists', save_folder='song_data',\n",
        "                   sr=16000, n_mels=128,\n",
        "                   n_fft=2048, hop_length=512):\n",
        "    \"\"\"This function creates the dataset given a folder\n",
        "     with the correct structure (artist_folder/artists/albums/*.mp3)\n",
        "    and saves it to a specified folder.\"\"\"\n",
        "\n",
        "    # get list of all artists\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "    artists = [path for path in os.listdir(artist_folder) if\n",
        "               os.path.isdir(os.path.join(artist_folder, path))]\n",
        "    print(\"artists : \", artists)\n",
        "\n",
        "    # iterate through all artists, albums, songs and find mel spectrogram\n",
        "    for artist in artists:\n",
        "        artist_path = os.path.join(artist_folder, artist)\n",
        "        artist_songs = os.listdir(artist_path)\n",
        "        print(\"songs : \", artist_songs)\n",
        "\n",
        "        for song in artist_songs:\n",
        "            song_path = os.path.join(artist_path, song)\n",
        "\n",
        "            # Create mel spectrogram and convert it to the log scale\n",
        "            y, sr = librosa.load(song_path, sr=sr)\n",
        "            S = librosa.feature.melspectrogram(y, sr=sr, n_mels=n_mels,\n",
        "                                                n_fft=n_fft,\n",
        "                                                hop_length=hop_length)\n",
        "            log_S = librosa.amplitude_to_db(S, ref=1.0)\n",
        "            data = (artist, log_S, song)\n",
        "\n",
        "            # Save each song\n",
        "            save_name = artist + '_%%-%%_' + song\n",
        "            with open(os.path.join(save_folder, save_name), 'wb') as fp:\n",
        "                dill.dump(data, fp)\n",
        "\n",
        "\n",
        "def load_dataset(song_folder_name='song_data',\n",
        "                 artist_folder='artists',\n",
        "                 nb_classes=20, random_state=42):\n",
        "    \"\"\"This function loads the dataset based on a location;\n",
        "     it returns a list of spectrograms\n",
        "     and their corresponding artists/song names\"\"\"\n",
        "\n",
        "    # Get all songs saved as numpy arrays in the given folder\n",
        "    song_list = os.listdir(song_folder_name)\n",
        "\n",
        "    # Load the list of artists\n",
        "    artist_list = os.listdir(artist_folder)\n",
        "\n",
        "    # select the appropriate number of classes\n",
        "    prng = RandomState(random_state)\n",
        "    artists = prng.choice(artist_list, size=nb_classes, replace=False)\n",
        "\n",
        "    # Create empty lists\n",
        "    artist = []\n",
        "    spectrogram = []\n",
        "    song_name = []\n",
        "\n",
        "    # Load each song into memory if the artist is included and return\n",
        "    for song in song_list:\n",
        "        with open(os.path.join(song_folder_name, song), 'rb') as fp:\n",
        "            loaded_song = dill.load(fp)\n",
        "        if loaded_song[0] in artists:\n",
        "            artist.append(loaded_song[0])\n",
        "            spectrogram.append(loaded_song[1])\n",
        "            song_name.append(loaded_song[2])\n",
        "\n",
        "    return artist, spectrogram, song_name\n",
        "\n",
        "\n",
        "def load_dataset_song_split(song_folder_name='song_data',\n",
        "                            artist_folder='artists',\n",
        "                            nb_classes=20,\n",
        "                            test_split_size=0.1,\n",
        "                            validation_split_size=0.1,\n",
        "                            random_state=42):\n",
        "    Y, X, S = load_dataset(song_folder_name=song_folder_name,\n",
        "                           artist_folder=artist_folder,\n",
        "                           nb_classes=nb_classes,\n",
        "                           random_state=random_state)\n",
        "    # train and test split\n",
        "    X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(\n",
        "        X, Y, S, test_size=test_split_size, stratify=Y,\n",
        "        random_state=random_state)\n",
        "\n",
        "    # Create a validation to be used to track progress\n",
        "    X_train, X_val, Y_train, Y_val, S_train, S_val = train_test_split(\n",
        "        X_train, Y_train, S_train, test_size=validation_split_size,\n",
        "        shuffle=True, stratify=Y_train, random_state=random_state)\n",
        "\n",
        "    return Y_train, X_train, S_train, \\\n",
        "           Y_test, X_test, S_test, \\\n",
        "           Y_val, X_val, S_val\n",
        "\n",
        "\n",
        "def slice_songs(X, Y, S, length=911):\n",
        "    \"\"\"Slices the spectrogram into sub-spectrograms according to length\"\"\"\n",
        "\n",
        "    # Create empty lists for train and test sets\n",
        "    artist = []\n",
        "    spectrogram = []\n",
        "    song_name = []\n",
        "\n",
        "    # Slice up songs using the length specified\n",
        "    for i, song in enumerate(X):\n",
        "        slices = int(song.shape[1] / length)\n",
        "        for j in range(slices - 1):\n",
        "            spectrogram.append(song[:, length * j:length * (j + 1)])\n",
        "            artist.append(Y[i])\n",
        "            song_name.append(S[i])\n",
        "\n",
        "    return np.array(spectrogram), np.array(artist), np.array(song_name)\n",
        "\n",
        "\n",
        "def predict_artist(model, X, Y, S,\n",
        "                   le, class_names,\n",
        "                   slices=None, verbose=False,\n",
        "                   ml_mode=False):\n",
        "    \"\"\"\n",
        "    This function takes slices of songs and predicts their output.\n",
        "    For each song, it votes on the most frequent artist.\n",
        "    \"\"\"\n",
        "    print(\"Test results when pooling slices by song and voting:\")\n",
        "    # Obtain the list of songs\n",
        "    songs = np.unique(S)\n",
        "\n",
        "    prediction_list = []\n",
        "    actual_list = []\n",
        "\n",
        "    # Iterate through each song\n",
        "    for song in songs:\n",
        "\n",
        "        # Grab all slices related to a particular song\n",
        "        X_song = X[S == song]\n",
        "        Y_song = Y[S == song]\n",
        "\n",
        "        # If not using full song, shuffle and take up to a number of slices\n",
        "        if slices and slices <= X_song.shape[0]:\n",
        "            X_song, Y_song = shuffle(X_song, Y_song)\n",
        "            X_song = X_song[:slices]\n",
        "            Y_song = Y_song[:slices]\n",
        "\n",
        "        # Get probabilities of each class\n",
        "        predictions = model.predict(X_song, verbose=0)\n",
        "\n",
        "        if not ml_mode:\n",
        "            # Get list of highest probability classes and their probability\n",
        "            class_prediction = np.argmax(predictions, axis=1)\n",
        "            class_probability = np.max(predictions, axis=1)\n",
        "\n",
        "            # keep only predictions confident about;\n",
        "            prediction_summary_trim = class_prediction[class_probability > 0.5]\n",
        "\n",
        "            # deal with edge case where there is no confident class\n",
        "            if len(prediction_summary_trim) == 0:\n",
        "                prediction_summary_trim = class_prediction\n",
        "        else:\n",
        "            prediction_summary_trim = predictions\n",
        "\n",
        "        # get most frequent class\n",
        "        prediction = stats.mode(prediction_summary_trim)[0][0]\n",
        "        actual = stats.mode(np.argmax(Y_song))[0][0]\n",
        "\n",
        "        # Keeping track of overall song classification accuracy\n",
        "        prediction_list.append(prediction)\n",
        "        actual_list.append(actual)\n",
        "\n",
        "        # Print out prediction\n",
        "        if verbose:\n",
        "            print(song)\n",
        "            print(\"Predicted:\", le.inverse_transform(prediction), \"\\nActual:\",\n",
        "                  le.inverse_transform(actual))\n",
        "            print('\\n')\n",
        "\n",
        "    # Print overall song accuracy\n",
        "    actual_array = np.array(actual_list)\n",
        "    prediction_array = np.array(prediction_list)\n",
        "    cm = confusion_matrix(actual_array, prediction_array)\n",
        "    plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
        "                          title='Confusion matrix for pooled results' +\n",
        "                                ' with normalization')\n",
        "    class_report = classification_report(actual_array, prediction_array,\n",
        "                                         target_names=class_names)\n",
        "    print(class_report)\n",
        "\n",
        "    class_report_dict = classification_report(actual_array, prediction_array,\n",
        "                                              target_names=class_names,\n",
        "                                              output_dict=True)\n",
        "    return (class_report, class_report_dict)\n",
        "\n",
        "\n",
        "def encode_labels(Y, le=None, enc=None):\n",
        "    \"\"\"Encodes target variables into numbers and then one hot encodings\"\"\"\n",
        "\n",
        "    # initialize encoders\n",
        "    N = Y.shape[0]\n",
        "\n",
        "    # Encode the labels\n",
        "    if le is None:\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        Y_le = le.fit_transform(Y).reshape(N, 1)\n",
        "    else:\n",
        "        Y_le = le.transform(Y).reshape(N, 1)\n",
        "\n",
        "    # convert into one hot encoding\n",
        "    if enc is None:\n",
        "        enc = preprocessing.OneHotEncoder()\n",
        "        Y_enc = enc.fit_transform(Y_le).toarray()\n",
        "    else:\n",
        "        Y_enc = enc.transform(Y_le).toarray()\n",
        "\n",
        "    # return encoders to re-use on other data\n",
        "    return Y_enc, le, enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zi3joJ5lBfI"
      },
      "source": [
        "# 구글 드라이브 연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8XR3w0LlEzB",
        "outputId": "c92474f3-6f17-4831-8e4a-50162684083d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKl4t2NMlIYt"
      },
      "source": [
        "%cd /content/drive/MyDrive/졸작/mp3파일/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTEaGDG1lWqt"
      },
      "source": [
        "# 데이터셋 만들기 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqjNArldqSCD",
        "outputId": "054ac2cc-5370-403c-e7f3-b2f5deea6f1c"
      },
      "source": [
        "forder_name = os.path.join(os.getcwd(), 'moods')\n",
        "os.listdir(forder_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dramatic',\n",
              " 'sadness',\n",
              " 'darkness',\n",
              " 'calm',\n",
              " 'funky',\n",
              " 'happiness',\n",
              " 'angry',\n",
              " 'brightness']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTqmcdYbnxPN"
      },
      "source": [
        "create_dataset(artist_folder=forder_name, save_folder='song_data',\n",
        "               sr=16000, n_mels=128, n_fft=2048,\n",
        "               hop_length=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLrD8Ml-ffl4"
      },
      "source": [
        "# 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwCBru35fiSG"
      },
      "source": [
        "def CRNN2D(X_shape, nb_classes):\n",
        "    '''\n",
        "    Model used for evaluation in paper. Inspired by K. Choi model in:\n",
        "    https://github.com/keunwoochoi/music-auto_tagging-keras/blob/master/music_tagger_crnn.py\n",
        "    '''\n",
        "\n",
        "    nb_layers = 4  # number of convolutional layers\n",
        "    nb_filters = [64, 128, 128, 128]  # filter sizes\n",
        "    kernel_size = (3, 3)  # convolution kernel size\n",
        "    activation = 'elu'  # activation function to use after each layer\n",
        "    pool_size = [(2, 2), (4, 2), (4, 2), (4, 2),\n",
        "                 (4, 2)]  # size of pooling area\n",
        "\n",
        "    # shape of input data (frequency, time, channels)\n",
        "    input_shape = (X_shape[1], X_shape[2], X_shape[3])\n",
        "    frequency_axis = 1\n",
        "    time_axis = 2\n",
        "    channel_axis = 3\n",
        "\n",
        "    # Create sequential model and normalize along frequency axis\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\n",
        "\n",
        "    # First convolution layer specifies shape\n",
        "    model.add(Conv2D(nb_filters[0], kernel_size=kernel_size, padding='same',\n",
        "                     data_format=\"channels_last\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(Activation(activation))\n",
        "    model.add(BatchNormalization(axis=channel_axis))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size[0], strides=pool_size[0]))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Add more convolutional layers\n",
        "    for layer in range(nb_layers - 1):\n",
        "        # Convolutional layer\n",
        "        model.add(Conv2D(nb_filters[layer + 1], kernel_size=kernel_size,\n",
        "                         padding='same'))\n",
        "        model.add(Activation(activation))\n",
        "        model.add(BatchNormalization(\n",
        "            axis=channel_axis))  # Improves overfitting/underfitting\n",
        "        model.add(MaxPooling2D(pool_size=pool_size[layer + 1],\n",
        "                               strides=pool_size[layer + 1]))  # Max pooling\n",
        "        model.add(Dropout(0.1))\n",
        "\n",
        "        # Reshaping input for recurrent layer\n",
        "    # (frequency, time, channels) --> (time, frequency, channel)\n",
        "    model.add(Permute((time_axis, frequency_axis, channel_axis)))\n",
        "    resize_shape = model.output_shape[2] * model.output_shape[3]\n",
        "    model.add(Reshape((model.output_shape[1], resize_shape)))\n",
        "\n",
        "    # recurrent layer\n",
        "    model.add(GRU(32, return_sequences=True))\n",
        "    model.add(GRU(32, return_sequences=False))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(nb_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxNDgJmQfUd7"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeWLij1S6NoC"
      },
      "source": [
        "def train_model(nb_classes=20,\n",
        "                slice_length=911,\n",
        "                artist_folder='artists',\n",
        "                song_folder='song_data',\n",
        "                plots=True,\n",
        "                train=True,\n",
        "                load_checkpoint=False,\n",
        "                save_metrics=True,\n",
        "                save_metrics_folder='metrics',\n",
        "                save_weights_folder='weights',\n",
        "                batch_size=16,\n",
        "                nb_epochs=200,\n",
        "                early_stop=10,\n",
        "                lr=0.0001,\n",
        "                album_split=False,\n",
        "                random_states=42):\n",
        "    \"\"\"\n",
        "    Main function for training the model and testing\n",
        "    \"\"\"\n",
        "\n",
        "    weights = os.path.join(save_weights_folder, str(nb_classes) +\n",
        "                           '_' + str(slice_length) + '_' + str(random_states))\n",
        "    os.makedirs(save_weights_folder, exist_ok=True)\n",
        "    os.makedirs(save_metrics_folder, exist_ok=True)\n",
        "\n",
        "    print(\"Loading dataset...\")\n",
        "\n",
        "    if not album_split:\n",
        "        # song split\n",
        "        Y_train, X_train, S_train, Y_test, X_test, S_test, \\\n",
        "        Y_val, X_val, S_val = \\\n",
        "            load_dataset_song_split(song_folder_name=song_folder,\n",
        "                                    artist_folder=artist_folder,\n",
        "                                    nb_classes=nb_classes,\n",
        "                                    random_state=random_states,\n",
        "                                    test_split_size=0.05,\n",
        "                                    validation_split_size=0.05)\n",
        "    else:\n",
        "        Y_train, X_train, S_train, Y_test, X_test, S_test, \\\n",
        "        Y_val, X_val, S_val = \\\n",
        "            load_dataset_album_split(song_folder_name=song_folder,\n",
        "                                     artist_folder=artist_folder,\n",
        "                                     nb_classes=nb_classes,\n",
        "                                     random_state=random_states)\n",
        "\n",
        "    print(\"Loaded and split dataset. Slicing songs...\")\n",
        "\n",
        "    # Create slices out of the songs\n",
        "    X_train, Y_train, S_train = slice_songs(X_train, Y_train, S_train,\n",
        "                                            length=slice_length)\n",
        "    X_val, Y_val, S_val = slice_songs(X_val, Y_val, S_val,\n",
        "                                      length=slice_length)\n",
        "    X_test, Y_test, S_test = slice_songs(X_test, Y_test, S_test,\n",
        "                                         length=slice_length)\n",
        "\n",
        "    print(\"Training set label counts:\", np.unique(Y_train, return_counts=True))\n",
        "\n",
        "    # Encode the target vectors into one-hot encoded vectors\n",
        "    Y_train, le, enc = encode_labels(Y_train)\n",
        "    Y_test, le, enc = encode_labels(Y_test, le, enc)\n",
        "    Y_val, le, enc = encode_labels(Y_val, le, enc)\n",
        "\n",
        "    # Reshape data as 2d convolutional tensor shape\n",
        "    X_train = X_train.reshape(X_train.shape + (1,))\n",
        "    X_val = X_val.reshape(X_val.shape + (1,))\n",
        "    X_test = X_test.reshape(X_test.shape + (1,))\n",
        "\n",
        "    # build the model\n",
        "    model = CRNN2D(X_train.shape, nb_classes=Y_train.shape[1])\n",
        "    #model = CRNN1D(X_train.shape, nb_classes=Y_train.shape[1])\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(lr=lr),\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # Initialize weights using checkpoint if it exists\n",
        "    if load_checkpoint:\n",
        "        print(\"Looking for previous weights...\")\n",
        "        if isfile(weights):\n",
        "            print('Checkpoint file detected. Loading weights.')\n",
        "            model.load_weights(weights)\n",
        "        else:\n",
        "            print('No checkpoint file detected.  Starting from scratch.')\n",
        "    else:\n",
        "        print('Starting from scratch (no checkpoint)')\n",
        "\n",
        "    checkpointer = ModelCheckpoint(filepath=weights,\n",
        "                                   verbose=1,\n",
        "                                   save_best_only=True,\n",
        "                                   monitor='val_accuracy')\n",
        "    earlystopper = EarlyStopping(monitor='val_loss', min_delta=0,\n",
        "                                 patience=early_stop, verbose=0, mode='auto')\n",
        "\n",
        "    # Train the model\n",
        "    if train:\n",
        "        print(\"Input Data Shape\", X_train.shape)\n",
        "        history = model.fit(X_train, Y_train, batch_size=batch_size,\n",
        "                            shuffle=True, epochs=nb_epochs,\n",
        "                            verbose=1, validation_data=(X_val, Y_val),\n",
        "                            callbacks=[checkpointer, earlystopper])\n",
        "        if plots:\n",
        "            plot_history(history)\n",
        "\n",
        "    # Load weights that gave best performance on validation set\n",
        "    model.load_weights(weights)\n",
        "    filename = os.path.join(save_metrics_folder, str(nb_classes) + '_'\n",
        "                            + str(slice_length)\n",
        "                            + '_' + str(random_states))\n",
        "\n",
        "    # Score test model\n",
        "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    y_score = model.predict_proba(X_test)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    y_predict = np.argmax(y_score, axis=1)\n",
        "    y_true = np.argmax(Y_test, axis=1)\n",
        "    cm = confusion_matrix(y_true, y_predict)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    class_names = np.arange(nb_classes)\n",
        "    class_names_original = le.inverse_transform(class_names)\n",
        "    plt.figure(figsize=(14, 14))\n",
        "    plot_confusion_matrix(cm, classes=class_names_original,\n",
        "                          normalize=True,\n",
        "                          title='Confusion matrix with normalization')\n",
        "    if save_metrics:\n",
        "        plt.savefig(filename + '.png', bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    plt.figure(figsize=(14, 14))\n",
        "\n",
        "    # Print out metrics\n",
        "    print('Test score/loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    print('\\nTest results on each slice:')\n",
        "    scores = classification_report(y_true, y_predict,\n",
        "                                   target_names=class_names_original)\n",
        "    scores_dict = classification_report(y_true, y_predict,\n",
        "                                        target_names=class_names_original,\n",
        "                                        output_dict=True)\n",
        "    print(scores)\n",
        "\n",
        "    # Predict artist using pooling methodology\n",
        "    pooling_scores, pooled_scores_dict = \\\n",
        "        predict_artist(model, X_test, Y_test, S_test,\n",
        "                       le, class_names=class_names_original,\n",
        "                       slices=None, verbose=False)\n",
        "\n",
        "    # Save metrics\n",
        "    if save_metrics:\n",
        "        plt.savefig(filename + '_pooled.png', bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "        with open(filename+'.txt', 'w') as f:\n",
        "            f.write(\"Training data shape:\" + str(X_train.shape))\n",
        "            f.write('\\nnb_classes: ' + str(nb_classes) +\n",
        "                    '\\nslice_length: ' + str(slice_length))\n",
        "            f.write('\\nweights: ' + weights)\n",
        "            f.write('\\nlr: ' + str(lr))\n",
        "            f.write('\\nTest score/loss: ' + str(score[0]))\n",
        "            f.write('\\nTest accuracy: ' + str(score[1]))\n",
        "            f.write('\\nTest results on each slice:\\n')\n",
        "            f.write(str(scores))\n",
        "            f.write('\\n\\n Scores when pooling song slices:\\n')\n",
        "            f.write(str(pooling_scores))\n",
        "\n",
        "    return (scores_dict, pooled_scores_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9XhsLbq0ebL"
      },
      "source": [
        "'''\n",
        "1s 32 frames\n",
        "3s 94 frames\n",
        "5s 157 frames\n",
        "6s 188 frames\n",
        "10s 313 frames\n",
        "20s 628 frames\n",
        "29.12s 911 frames\n",
        "'''\n",
        "\n",
        "#slice_lengths = [911, 628, 313, 157, 94, 32]\n",
        "slice_lengths = [94] # 3 sec\n",
        "#random_state_list = [0, 21, 42]\n",
        "random_state_list = [21]\n",
        "iterations = 1\n",
        "summary_metrics_output_folder = 'trials_song_split'\n",
        "\n",
        "original_folder = os.path.join(os.getcwd(), 'moods')\n",
        "processed_folder = os.path.join(os.getcwd(), 'song_data')\n",
        "\n",
        "for slice_len in slice_lengths:\n",
        "    scores = []\n",
        "    pooling_scores = []\n",
        "    for i in range(iterations):\n",
        "        print(\"Hello! Iteration : \", i)\n",
        "        score, pooling_score = train_model(\n",
        "            nb_classes=8,\n",
        "            slice_length=slice_len,\n",
        "            artist_folder=original_folder,\n",
        "            song_folder=processed_folder,\n",
        "            lr=0.001,\n",
        "            train=True,\n",
        "            load_checkpoint=True,\n",
        "            plots=False,\n",
        "            album_split=False,\n",
        "            random_states=random_state_list[i],\n",
        "            save_metrics=True,\n",
        "            save_metrics_folder='metrics_song_split',\n",
        "            save_weights_folder='weights_song_split')\n",
        "\n",
        "        scores.append(score['weighted avg'])\n",
        "        pooling_scores.append(pooling_score['weighted avg'])\n",
        "        gc.collect()\n",
        "\n",
        "    os.makedirs(summary_metrics_output_folder, exist_ok=True)\n",
        "\n",
        "    pd.DataFrame(scores).to_csv(\n",
        "        '{}/{}_score.csv'.format(summary_metrics_output_folder, slice_len))\n",
        "\n",
        "    pd.DataFrame(pooling_scores).to_csv(\n",
        "        '{}/{}_pooled_score.csv'.format(\n",
        "            summary_metrics_output_folder, slice_len))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}